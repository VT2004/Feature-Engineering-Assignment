{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2I5R7937rc1l4UY8oU68h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1. What is a parameter?"],"metadata":{"id":"Mlb34IizriHh"}},{"cell_type":"markdown","source":["In machine learning, a parameter refers to a value that is learned by the model during training. Parameters define the model's structure and behavior, and they are adjusted iteratively to minimize the error or loss function."],"metadata":{"id":"lZmZHTuZriJ-"}},{"cell_type":"markdown","source":["Q2. What is correlation?\n","What does negative correlation mean?"],"metadata":{"id":"OEEQfCQhriMr"}},{"cell_type":"markdown","source":["Correlation is a statistical measure that quantifies the strength and direction of a relationship between two variables. It is represented by a value called the correlation coefficient (r), which ranges from -1 to +1:\n","\n","\n","+1: Perfect positive correlation (as one variable increases, the other also increases).\n","\n","0: No correlation (no relationship between the variables).\n","\n","-1: Perfect negative correlation (as one variable increases, the other decreases).\n","\n","Negative Correlation:\n","\n","A negative correlation means that as one variable increases, the other decreases. The correlation coefficient in this case will be less than 0 but greater than or equal to -1.\n","\n","Example:\n","As hours spent watching TV increase, test scores tend to decrease."],"metadata":{"id":"b_LKEdUlriPP"}},{"cell_type":"markdown","source":["Q3. Define Machine Learning. What are the main components in Machine Learning?"],"metadata":{"id":"PvbQRMs0riR7"}},{"cell_type":"markdown","source":["Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that can learn and improve from experience (data) without being explicitly programmed. It enables computers to find patterns, make decisions, or predict outcomes based on input data.\n","\n"],"metadata":{"id":"brss-Iw9riUX"}},{"cell_type":"markdown","source":["Main Components in Machine Learning:\n","\n","1. Dataset:\n","\n","The collection of data used to train and evaluate the model.\n","\n","Example: Images, text, or numerical data.\n","2. Features:\n","\n","\n","The input variables or attributes that describe the data.\n","\n","Example: In a house price prediction model, features could be \"size,\" \"location,\" and \"number of rooms.\"\n","\n","3. Model:\n","\n","The mathematical algorithm or framework that processes the data to make predictions or decisions.\n","\n","Example: Linear regression, decision trees, neural networks.\n","4. Training:\n","\n","The process of feeding the model with data and adjusting its parameters to minimize the error.\n","\n","Example: Gradient descent is a common method used for training.\n","5. Evaluation:\n","\n","Assessing the model's performance using metrics like accuracy, precision, recall, or RMSE on a test dataset.\n","\n","6. Hyperparameters:\n","\n","The configuration settings defined before training that control the model's behavior.\n","\n","Example: Learning rate, number of layers in a neural network.\n","7. Loss Function:\n","\n","A mathematical function that quantifies the difference between the predicted output and the actual output.\n","\n","Example: Mean Squared Error for regression tasks.\n"],"metadata":{"id":"mFGuTwyxriXj"}},{"cell_type":"markdown","source":["Q4. How does loss value help in determining whether the model is good or not?"],"metadata":{"id":"6KsQ2WPhuD8z"}},{"cell_type":"markdown","source":["The loss value measures how far the model's predictions are from the actual outcomes. It is a numerical representation of the model's error during training or testing. A lower loss value indicates better performance, as it suggests the model is making more accurate predictions."],"metadata":{"id":"dGjGXkGTuD_E"}},{"cell_type":"markdown","source":["Key Points:\n","\n","1. Interpreting Loss:\n","\n","A low loss value: The model is performing well, with minimal error.\n","\n","A high loss value: The model is not performing well, with significant error.\n","2. Improving the Model:\n","\n","A decreasing loss value during training shows that the model is learning and improving.\n","\n","A consistently high or fluctuating loss value indicates issues like:\n","\n","Poor model architecture.\n","\n","Overfitting or underfitting.\n","\n","Insufficient or irrelevant data.\n","\n","3. Loss vs. Accuracy:\n","\n","Loss focuses on the magnitude of errors made by the model.\n","\n","Metrics like accuracy evaluate how often the model's predictions are correct. A low loss often correlates with high accuracy.\n","\n","Example:\n","\n","For a regression task:\n","\n","\n","Loss value of 0.1 (Mean Squared Error): The predictions are very close to actual values.\n","\n","Loss value of 10.0: The model needs improvement, as predictions are far from the actual values.\n"],"metadata":{"id":"dxsFdowQuEBp"}},{"cell_type":"markdown","source":["Q5. What are continuous and categorical variables?"],"metadata":{"id":"LlGGyI7guED-"}},{"cell_type":"markdown","source":["1. Continuous Variables:\n","\n","Definition: Variables that can take an infinite number of values within a range.\n","\n","Examples:\n","\n","Height (e.g., 160.5 cm, 175.2 cm),\n","Temperature (e.g., 37.5°C, 42.3°C),\n","Price (e.g., ₹299.99, ₹1000.50).\n","\n","Characteristics:\n","\n","Measured on a scale.\n","\n","Often represented using floating-point numbers.\n","\n","2. Categorical Variables:\n","\n","Definition: Variables that represent categories or groups, often taking a limited number of distinct values.\n","\n","Examples:\n","\n","Gender (Male, Female, Other),\n","Color (Red, Blue, Green),\n","Product type (Electronics, Furniture, Clothing).\n","\n","Characteristics:\n","\n","Can be nominal (no order) or ordinal (ordered categories).\n","\n","Often represented using strings or integers as labels.\n"],"metadata":{"id":"0qGxaV4vuEGq"}},{"cell_type":"markdown","source":["Q6. How do we handle categorical variables in Machine Learning? What are the common techniques?"],"metadata":{"id":"G9NWdRnMuEJI"}},{"cell_type":"markdown","source":["Categorical variables need to be converted into numerical representations for use in machine learning models. The common techniques for handling them are:\n","\n","1. Label Encoding:\n","Assigns a unique integer to each category.\n","\n","Example:\n","\n","Categories: [\"Red\", \"Green\", \"Blue\"]\n","\n","Encoded: [0, 1, 2]\n","\n","When to Use: Suitable for ordinal categories (e.g., low, medium, high) where order matters.\n","\n","2. One-Hot Encoding:\n","Converts each category into a separate binary column.\n","\n","Example:\n","\n","Categories: [\"Red\", \"Green\", \"Blue\"]\n","\n","Encoded:\n","\n","\n"],"metadata":{"id":"4ZvxFRGWuELj"}},{"cell_type":"code","source":["#Red   Green   Blue\n","#1      0       0\n","#0      1       0\n","#0      0       1\n"],"metadata":{"id":"v03J-3dLv1ju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When to Use: For nominal categories (e.g., city names, product types) where order doesn't matter.\n","\n","3. Ordinal Encoding:\n","Assigns integers to categories based on their order.\n","\n","Example:\n","\n","Categories: [\"Low\", \"Medium\", \"High\"]\n","\n","Encoded: [0, 1, 2]\n","\n","When to Use: For ordered categorical variables.\n","\n","4. Frequency or Count Encoding:\n","Replaces each category with the frequency or count of occurrences in the dataset.\n","\n","Example:\n","\n","Categories: [\"A\", \"B\", \"B\", \"C\", \"C\", \"C\"]\n","\n","Encoded: [1, 2, 2, 3, 3, 3]\n","\n","When to Use: For datasets where frequency provides meaningful information.\n","\n","5. Target Encoding:\n","Replaces categories with the mean of the target variable for each category.\n","\n","Example:\n","\n","Categories: [\"A\", \"B\", \"B\", \"C\"]\n","\n","Target: [10, 20, 20, 30]\n","\n","Encoded: [10, 20, 20, 30] (mean target value for each category)\n","\n","When to Use: In supervised learning tasks.\n","\n","6. Binary Encoding:\n","Converts categories into binary code representations.\n","\n","Example:\n","\n","Categories: [\"Red\", \"Green\", \"Blue\"]\n","\n","Encoded:\n","\n"],"metadata":{"id":"x0Iv70LLuEOP"}},{"cell_type":"code","source":["#Red   -> 001\n","#Green -> 010\n","#Blue  -> 011\n","\n","#When to Use: For high-cardinality categorical variables (many unique categories)"],"metadata":{"id":"87sAlEGxw1ej"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q7. What do you mean by training and testing a dataset?\n","\n"],"metadata":{"id":"HUjWk--ZuEQt"}},{"cell_type":"markdown","source":["In machine learning, a dataset is typically divided into two (or more) subsets to evaluate the model's performance and generalizability.\n","\n","\n","1. Training Dataset:\n","The subset of data used to train the model. The model learns patterns, relationships, and parameters (like weights in neural networks) from this data.\n","\n","Purpose: To fit the model and minimize the error (loss).\n","\n","Example: For predicting house prices, training data would include features (e.g., size, location) and corresponding prices.\n","\n","2. Testing Dataset:\n","The subset of data used to evaluate the model's performance after training.\n","\n","Purpose: To test how well the model can generalize to unseen data and avoid overfitting.\n","\n","Example: For the same house price prediction model, the testing data will consist of new houses (features) whose prices are hidden from the model during training.\n","\n","Key Points:\n","\n","1. Splitting the Dataset:\n","\n","\n","Common practice: Split data into 80% training and 20% testing, but the ratio can vary (e.g., 70/30).\n","\n","Use libraries like train_test_split in Python for this.\n","\n","2. Evaluation Metrics:\n","\n","Use metrics like accuracy, precision, recall, or RMSE on the test set to check performance.\n","\n","3. Validation (Optional):\n","\n","Sometimes, a validation set is also created to fine-tune hyperparameters without touching the test set.\n"],"metadata":{"id":"72gXvyMQuETi"}},{"cell_type":"markdown","source":["Q8. What is sklearn.preprocessing?"],"metadata":{"id":"nRHobRCuxqK2"}},{"cell_type":"markdown","source":["sklearn.preprocessing is a module in Scikit-learn that provides tools for preprocessing and transforming data to make it suitable for machine learning models. Preprocessing ensures that the data is in the right format, scaled, or encoded for optimal model performance.\n","\n"],"metadata":{"id":"iNiyane5xqNT"}},{"cell_type":"markdown","source":["Q9. What is a Test set?"],"metadata":{"id":"ISTPwgIfxqPx"}},{"cell_type":"markdown","source":["A test set is a subset of the dataset that is used to evaluate the performance of a trained machine learning model. It contains data that the model has not seen during training, allowing us to assess how well the model generalizes to unseen data.\n","\n","Key Characteristics of a Test Set:\n","\n","1. Purpose:\n","\n","To measure the model's generalization ability.\n","\n","To provide an unbiased evaluation of the model's performance.\n","2. Data Splitting:\n","\n","A typical dataset is split into:\n","\n","Training set: Used to train the model (e.g., 80% of data).\n","\n","Test set: Used to evaluate the model (e.g., 20% of data).\n","\n","Use functions like train_test_split from Scikit-learn to perform the split.\n","\n","3. Evaluation:\n","\n","Performance metrics such as accuracy, precision, recall, or RMSE are calculated on the test set.\n","\n","Example: If the test set contains unseen images, the model predicts their labels, and the results are compared to the true labels.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FIOVOJL_xqR8"}},{"cell_type":"markdown","source":["Q10. How do we split data for model fitting (training and testing) in Python?\n","How do you approach a Machine Learning problem?"],"metadata":{"id":"i5e1qCOdxqUM"}},{"cell_type":"markdown","source":["To split data for training and testing in Python, we commonly use train_test_split from Scikit-learn, which randomly divides the dataset into two subsets: one for training and one for testing.\n","\n","Example:"],"metadata":{"id":"IG_g7Nv3xqWn"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Example dataset\n","X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])  # Features\n","y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Target\n","\n","# Splitting the data into 80% training and 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training Data (Features):\\n\", X_train)\n","print(\"Testing Data (Features):\\n\", X_test)\n","print(\"Training Data (Target):\\n\", y_train)\n","print(\"Testing Data (Target):\\n\", y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncbpwoEiyphB","executionInfo":{"status":"ok","timestamp":1733032694814,"user_tz":-330,"elapsed":6306,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"ef4cbf6f-d5d8-485e-ee51-070432f0329a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data (Features):\n"," [[ 6]\n"," [ 1]\n"," [ 8]\n"," [ 3]\n"," [10]\n"," [ 5]\n"," [ 4]\n"," [ 7]]\n","Testing Data (Features):\n"," [[9]\n"," [2]]\n","Training Data (Target):\n"," [ 6  1  8  3 10  5  4  7]\n","Testing Data (Target):\n"," [9 2]\n"]}]},{"cell_type":"markdown","source":["How to Approach a Machine Learning Problem:\n","\n","1. Define the Problem:\n","\n","Understand the goal of the problem (e.g., classification, regression, clustering).\n","Identify the input data (features) and the output (target/labels).\n","2. Data Collection:\n","\n","Gather or load the data from various sources (e.g., CSV, databases, APIs).\n","3. Data Preprocessing:\n","\n","Handle missing values: Fill or remove missing data.\n","\n","Encoding categorical data: Convert non-numeric data into numerical form.\n","\n","Scaling/normalizing data: Standardize features for certain models (e.g., StandardScaler).\n","\n","Splitting data: Divide data into training and test sets (e.g., 80/20 split).\n","4. Choose a Model:\n","\n","Select a suitable model based on the problem (e.g., linear regression, decision trees, random forest, neural networks).\n","5. Train the Model:\n","\n","Fit the model to the training data using the chosen algorithm.\n","Example: model.fit(X_train, y_train).\n","6. Model Evaluation:\n","\n","Evaluate the model's performance on the test set using metrics like accuracy, precision, recall, or RMSE.\n","Example: model.score(X_test, y_test) for regression or accuracy_score(y_test, predictions) for classification.\n","7. Hyperparameter Tuning:\n","\n","Optimize the model by adjusting hyperparameters (e.g., using grid search or random search).\n","8. Model Validation:\n","\n","Optionally, use cross-validation to assess model performance across different subsets of the data.\n","9. Deploy the Model:\n","\n","Once the model is trained and evaluated, deploy it to make predictions on new data.\n"],"metadata":{"id":"RGc3iwLzzeBw"}},{"cell_type":"markdown","source":["Q11. Why do we have to perform EDA before fitting a model to the data?"],"metadata":{"id":"yMBHw-ihzeEv"}},{"cell_type":"markdown","source":["Exploratory Data Analysis (EDA) is a crucial step in the data science and machine learning process because it helps you understand the dataset better and prepares the data for modeling. Here’s why EDA is important:\n","\n","1. Understanding the Data:\n","\n","Summary Statistics: EDA provides insights into the data distribution, central tendency (mean, median), spread (variance, standard deviation), and relationships between variables. This helps you understand the basic characteristics of the dataset.\n","\n","Example: Knowing the range of values can help you decide whether normalization or scaling is necessary.\n","\n","Data Types: It helps you identify the types of variables (categorical, continuous, etc.), which is important for deciding how to treat them during preprocessing.\n","\n","Example: Categorical features may need encoding (e.g., one-hot encoding), while continuous features may need scaling or normalization.\n","\n","2. Handling Missing Data:\n","\n","Identify Missing Values: EDA helps you locate missing or null values in your data. Missing values need to be handled before training a model, either by imputing them or removing rows/columns.\n","\n","Example: A dataset with missing values in important features might lead to inaccurate model predictions.\n","\n","3. Detecting Outliers:\n","\n","Outlier Detection: EDA allows you to identify outliers that might skew your model's performance. These outliers could be legitimate values or data entry errors.\n","\n","Example: Outliers in financial data could lead to predictions that don’t generalize well.\n","\n","Decide on Treatment: Depending on your findings, you can decide whether to remove, cap, or transform outliers.\n","\n","4. Visualizing Data Distribution:\n","\n","Data Distribution: Visualizing the data through plots (e.g., histograms, box plots) helps you understand the distribution of the variables, such as whether they follow a normal distribution or are skewed.\n","\n","Example: If a variable has a skewed distribution, you might apply transformations (e.g., log transformation) to improve the model's performance.\n","\n","Correlations: EDA allows you to identify relationships between variables (e.g., using heatmaps or scatter plots) to see if features are correlated.\n","Highly correlated features may lead to multicollinearity, which can degrade model performance.\n","\n","Example: Correlation between two features may suggest redundancy, which could be addressed by dropping one of them.\n","\n","5. Feature Engineering Insights:\n","\n","Generate New Features: Based on EDA, you may identify opportunities to create new features or drop irrelevant ones. This can significantly improve model performance.\n","\n","Example: From a timestamp, you might extract features like day of the week, month, or hour for time series predictions.\n","\n","Transform Features: Insights from visualizations (e.g., skewed features) may suggest the need for feature transformations (e.g., scaling or normalization).\n","\n","6. Model Selection:\n","\n","Choosing the Right Model: Understanding the data distribution and relationships can help in selecting an appropriate machine learning model.\n","\n","For instance:\n","\n","If your data has a linear relationship, linear regression or a simple decision tree might be appropriate.\n","\n","If your data has complex patterns, you might choose more sophisticated models like Random Forest or Gradient Boosting.\n"],"metadata":{"id":"PYOpLu6DzeHL"}},{"cell_type":"markdown","source":["Q12. What is correlation?"],"metadata":{"id":"nXT-_9VJzeJw"}},{"cell_type":"markdown","source":["Correlation is a statistical measure that describes the relationship between two or more variables. It indicates whether and how strongly pairs of variables are related. Correlation can help you understand the degree to which one variable changes when another variable changes.\n","\n"],"metadata":{"id":"wW9JH42n09L9"}},{"cell_type":"markdown","source":["Q13. What does negative correlation mean?"],"metadata":{"id":"XsQ_nrYk09Oj"}},{"cell_type":"markdown","source":["A negative correlation means that as one variable increases, the other decreases. The correlation coefficient in this case will be less than 0 but greater than or equal to -1.\n","\n","Example: As hours spent watching TV increase, test scores tend to decrease.\n","\n"],"metadata":{"id":"zJ2QDw9S09Q_"}},{"cell_type":"markdown","source":["Q14. How can you find correlation between variables in Python?"],"metadata":{"id":"cXRSrWRP09TE"}},{"cell_type":"markdown","source":["In Python, you can use several libraries to calculate the correlation between variables. The most common libraries for this task are Pandas and NumPy. Here's how you can calculate correlation:\n","\n","1. Using Pandas:\n","Pandas provides the .corr() method to calculate the correlation matrix of numeric columns in a DataFrame."],"metadata":{"id":"Sy27etMK1Udc"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data\n","data = {'Height': [160, 165, 170, 175, 180],\n","        'Weight': [55, 60, 65, 70, 75],\n","        'Age': [25, 30, 35, 40, 45]}\n","\n","df = pd.DataFrame(data)\n","\n","# Calculate the correlation matrix\n","correlation_matrix = df.corr()\n","\n","# Display the correlation matrix\n","print(correlation_matrix)\n","\n","#.corr() computes the Pearson correlation coefficient by default between each pair of numeric columns.\n","#It returns a correlation matrix where each value represents the correlation between two variables.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTowwNlI1g57","executionInfo":{"status":"ok","timestamp":1733033419592,"user_tz":-330,"elapsed":576,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"77f047b4-d4bb-47d1-edf0-8aef39cb8154"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["        Height  Weight  Age\n","Height     1.0     1.0  1.0\n","Weight     1.0     1.0  1.0\n","Age        1.0     1.0  1.0\n"]}]},{"cell_type":"markdown","source":["2. Using NumPy:\n","You can use NumPy's np.corrcoef() function to calculate the correlation between two variables."],"metadata":{"id":"SNEOlIug1UgB"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Sample data\n","height = np.array([160, 165, 170, 175, 180])\n","weight = np.array([55, 60, 65, 70, 75])\n","\n","# Calculate correlation coefficient\n","correlation = np.corrcoef(height, weight)\n","\n","print(correlation)\n","\n","#np.corrcoef() returns a 2x2 correlation matrix where:\n","#The diagonal values represent the correlation of a variable with itself (which is always 1).\n","#The off-diagonal values represent the correlation between the two variables."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2WKIa33v1vCB","executionInfo":{"status":"ok","timestamp":1733033466966,"user_tz":-330,"elapsed":405,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"e5bf1c7c-d967-43b2-cf6b-42ca978a2590"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 1.]\n"," [1. 1.]]\n"]}]},{"cell_type":"markdown","source":["Q15. What is causation? Explain difference between correlation and causation with an example."],"metadata":{"id":"lE_SaCq01Uij"}},{"cell_type":"markdown","source":["Causation refers to a relationship where one event or variable directly causes another to happen. In other words, a causal relationship means that a change in one variable directly leads to a change in another.\n","\n","For causation, there is a clear mechanism or reason why one variable influences the other, often based on scientific principles or logical reasoning.\n","\n","Difference Between Correlation and Causation\n","\n","While both correlation and causation describe relationships between two variables, the key difference lies in the nature of the relationship:\n","\n","1. Correlation:\n","\n","Correlation means that two variables move together in some way, but this does not imply that one causes the other.\n","\n","A correlation simply shows a statistical relationship without explaining why the relationship exists.\n","\n","It can be positive, negative, or even neutral.\n","2. Causation:\n","\n","Causation implies that one variable directly causes the change in the other.\n","Causation goes beyond statistical relationship and identifies a cause-effect relationship.\n"],"metadata":{"id":"FHQC8yay1UlA"}},{"cell_type":"markdown","source":["\n","1. Example of Correlation (but not Causation):\n","\n","Ice Cream Sales and Drowning Deaths:\n","\n","There's a correlation between ice cream sales and drowning deaths, i.e., both tend to increase during the summer months.\n","However, ice cream sales do not cause drowning deaths.\n","\n","The true cause is likely the warmer weather: people buy more ice cream and also tend to swim more, which increases the chances of drowning accidents.\n","\n","Correlation: Ice cream sales and drowning deaths both increase in summer.\n","\n","Causation?: No, the warm weather is the common factor causing both.\n","\n","2. Example of Causation:\n","\n","Smoking and Lung Cancer:\n","There is strong evidence that smoking directly causes lung cancer.\n","The cause is chemicals in cigarette smoke that damage the lungs and increase the risk of cancer.\n","\n","Causation: Smoking causes an increased risk of lung cancer.\n"],"metadata":{"id":"qMqDs7h61Une"}},{"cell_type":"markdown","source":["Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example."],"metadata":{"id":"WbEFomjb1UsJ"}},{"cell_type":"markdown","source":["In machine learning, an optimizer is an algorithm or method used to minimize or maximize an objective function, such as the loss function in training a model. The goal of the optimizer is to adjust the model's parameters (weights) in such a way that the model's error is minimized (or the objective function is maximized), improving the model's predictions.\n","\n","Different Types of Optimizers\n","\n","There are several types of optimizers used in machine learning and deep learning. Some common ones include:\n","\n","\n","1. Gradient Descent:\n","\n","Type: First-order optimization\n","\n","Description: The most basic and commonly used optimization technique. It adjusts the parameters in the direction of the negative gradient of the loss function with respect to the model's parameters.\n","\n","Example: Training a linear regression model using gradient descent\n","\n","2. Stochastic Gradient Descent (SGD):\n","\n","Type: First-order optimization\n","\n","Description: A variant of Gradient Descent where instead of using the entire dataset to calculate the gradient, it uses a random subset (mini-batch or single data point). This makes the optimizer faster but noisier, which can help escape local minima.\n","\n","Example: Training deep neural networks using small batches of data at each step.\n","\n","\n","3. Mini-Batch Gradient Descent:\n","\n","Type: First-order optimization\n","\n","Description: A compromise between Batch Gradient Descent (which uses the entire dataset) and Stochastic Gradient Descent (which uses one data point). Mini-batch uses a subset of the dataset to calculate the gradient, which allows for faster convergence and smoother updates.\n","\n","Example: Most deep learning frameworks use mini-batch gradient descent for training large models with large datasets.\n","\n","4. Momentum:\n","\n","Type: First-order optimization\n","\n","Description: Momentum helps the optimizer converge faster by adding a fraction of the previous update to the current update. This allows the optimizer to \"build momentum\" and escape local minima more easily.\n","\n","Example: Used in conjunction with SGD to stabilize and accelerate convergence.\n","\n","5. RMSprop (Root Mean Square Propagation):\n","\n","Type: Adaptive learning rate\n","\n","Description: An adaptive learning rate method that adjusts the learning rate based on the recent average of squared gradients. It helps in dealing with gradients that have large variations by normalizing the updates.\n","\n","Example: Often used for training recurrent neural networks (RNNs).\n","\n","6. Adam (Adaptive Moment Estimation):\n","\n","Type: Adaptive learning rate\n","\n","Description: Combines the ideas of Momentum and RMSprop. Adam computes adaptive learning rates for each parameter by maintaining both the first moment (mean) and second moment (uncentered variance) of the gradients.\n","\n","Example: Popular in training deep neural networks, especially for complex tasks like image recognition.\n","\n","7. Adagrad (Adaptive Gradient Algorithm):\n","\n","Type: Adaptive learning rate\n","\n","Description: Adagrad adjusts the learning rate for each parameter based on the historical sum of squared gradients. It performs well for sparse data but can result in the learning rate becoming too small after many updates.\n","\n","Example: Suitable for sparse datasets such as text classification.\n","\n","8. Nadam (Nesterov-accelerated Adaptive Moment Estimation):\n","\n","\n","Type: Adaptive learning rate\n","\n","Description: Nadam is a combination of Adam and Nesterov momentum, which incorporates Nesterov's momentum into the Adam algorithm to improve performance.\n","\n","Example: Used for large-scale deep learning models and is more efficient than vanilla Adam in certain cases.\n"],"metadata":{"id":"5Z_ynCcz1Uud"}},{"cell_type":"markdown","source":["Q17. What is sklearn.linear_model?"],"metadata":{"id":"p0ejcqj_1Uw-"}},{"cell_type":"markdown","source":["The module sklearn.linear_model in scikit-learn provides a collection of linear models for regression, classification, and other machine learning tasks. These models assume a linear relationship between the input features (independent variables) and the output (dependent variable), and they are used in a variety of machine learning algorithms."],"metadata":{"id":"Zg-QTLfk1UzI"}},{"cell_type":"markdown","source":["Q18. What does model.fit() do? What arguments must be given?"],"metadata":{"id":"EsPvWsyw5cHO"}},{"cell_type":"markdown","source":["In machine learning, model.fit() is a method used to train a machine learning model on a given dataset. The method allows the model to learn the relationship between the input features (independent variables) and the target variable (dependent variable) from the provided data. The model's parameters are adjusted to minimize the error between the predicted output and the actual output.\n","\n","For supervised learning, the fit() method is used to learn from labeled data (data with known outcomes), and for unsupervised learning, it is used to learn from unlabeled data.\n","\n","\n","What arguments must be given to model.fit()?\n","\n","The arguments required by the fit() method generally depend on the specific type of model and the problem you're trying to solve. However, for most supervised learning tasks, the two key arguments that are typically passed to fit() are:\n","\n","\n","1. X (Features/Input data):\n","\n","Type: 2D array-like (e.g., NumPy array, Pandas DataFrame, or list).\n","\n","Description: This is the feature matrix, which contains the input data that the model will learn from. Each row represents a data point (sample), and each column represents a feature (attribute).\n","\n","2. y (Target/Labels):\n","\n","Type: 1D array-like (e.g., NumPy array, Pandas Series, or list).\n","\n","Description: This is the target variable, which contains the actual output or label that the model is trying to predict. It is a vector of values corresponding to the rows in the feature matrix X.\n","\n","Syntax:\n","model.fit(X, y)\n","\n","Where:\n","\n","X is the feature matrix.\n","y is the target values.\n"],"metadata":{"id":"3XUxezW95cJ8"}},{"cell_type":"markdown","source":["Q19. What does model.predict() do? What arguments must be given?"],"metadata":{"id":"w93yVtnR5_2R"}},{"cell_type":"markdown","source":["\n","The model.predict() method is used to make predictions on new data using a trained machine learning model. After the model has been trained using the fit() method, predict() allows you to apply the learned patterns to predict outcomes for unseen data.\n","\n","\n","In the context of supervised learning:\n","\n","\n","For regression tasks, predict() will output continuous values (e.g., predicted house prices).\n","\n","For classification tasks, predict() will output discrete class labels (e.g., whether an email is spam or not).\n","\n","What arguments must be given to model.predict()?\n","\n","The key argument required by predict() is:\n","\n","1. X (Features/Input data):\n","\n","Type: 2D array-like (e.g., NumPy array, Pandas DataFrame, or list).\n","\n","Description: This is the feature matrix that contains the new, unseen data on which you want to make predictions. The number of columns (features) in X should match the number of features used when the model was trained with model.fit().\n","\n","Syntax:\n","\n","model.predict(X)\n","\n","Where:\n","\n","X is the input data you want to make predictions on.\n"],"metadata":{"id":"Q1Hxrnz55_4j"}},{"cell_type":"markdown","source":["Q20. What are continuous and categorical variables?"],"metadata":{"id":"HGRnEo5d5_7i"}},{"cell_type":"markdown","source":["1. Continuous Variables:\n","Definition: Variables that can take an infinite number of values within a range.\n","\n","Examples:\n","\n","Height (e.g., 160.5 cm, 175.2 cm), Temperature (e.g., 37.5°C, 42.3°C), Price (e.g., ₹299.99, ₹1000.50).\n","\n","Characteristics:\n","\n","Measured on a scale.\n","\n","Often represented using floating-point numbers.\n","\n","2. Categorical Variables:\n","Definition: Variables that represent categories or groups, often taking a limited number of distinct values.\n","\n","Examples:\n","\n","Gender (Male, Female, Other), Color (Red, Blue, Green), Product type (Electronics, Furniture, Clothing).\n","\n","Characteristics:\n","\n","Can be nominal (no order) or ordinal (ordered categories).\n","\n","Often represented using strings or integers as labels."],"metadata":{"id":"PW6lY1Bk6xP0"}},{"cell_type":"markdown","source":["Q21. What is feature scaling? How does it help in Machine Learning?"],"metadata":{"id":"G9yyOSOX7WO2"}},{"cell_type":"markdown","source":["Feature scaling is a technique used to normalize or standardize the range of independent variables (features) in a dataset. The goal of feature scaling is to ensure that all features contribute equally to the model, preventing any one feature from disproportionately influencing the model due to differences in the magnitude of its values.\n","\n","Feature scaling transforms the data so that the features are on a similar scale, which is particularly important for algorithms that rely on distance calculations (such as k-nearest neighbors, SVMs, and gradient descent-based models) or those sensitive to the range of features (like linear regression or neural networks).\n","\n","\n","How Does Feature Scaling Help in Machine Learning?\n","\n","Improves Model Performance:\n","\n","1. Gradient Descent: Many machine learning algorithms (like logistic regression, neural networks, etc.) use optimization techniques like gradient descent. When features have very different scales, gradient descent can struggle to converge efficiently. Feature scaling helps the optimization process by ensuring that all features are treated on an equal footing.\n","2. Prevents Bias Toward Larger-Scale Features:\n","\n","If features have different units or ranges (e.g., age in years vs. income in thousands of dollars), the model might give more importance to the feature with a larger numerical range. Feature scaling ensures that each feature contributes equally to the model, preventing this bias.\n","3. Distance-Based Algorithms:\n","\n","K-nearest neighbors (KNN), Support Vector Machines (SVM), and K-means clustering rely on calculating distances (like Euclidean distance) between data points. If features are on different scales, the distance calculation can be dominated by features with larger numerical values, which can affect the accuracy of the model.\n","4. Improves Convergence Speed:\n","\n","For some algorithms, like gradient-based methods, feature scaling can help the model converge faster to the optimal solution."],"metadata":{"id":"d98lQwkE7WRi"}},{"cell_type":"markdown","source":["Q22. How do we perform scaling in Python?"],"metadata":{"id":"i58ehBBC8Yv1"}},{"cell_type":"markdown","source":["In Python, feature scaling can be easily performed using the sklearn.preprocessing module from the scikit-learn library. The most common techniques for scaling are Min-Max scaling (Normalization) and Standardization (Z-score normalization).\n","\n","1. Min-Max Scaling (Normalization)\n","\n","Min-Max scaling transforms the features to a fixed range, usually [0, 1]. You can use MinMaxScaler from sklearn.preprocessing to achieve this."],"metadata":{"id":"x8p0k4OL8Yyq"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","# Sample data (features with different scales)\n","X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Fit and transform the data\n","X_scaled = scaler.fit_transform(X)\n","\n","print(X_scaled)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_sZneC_5Je0","executionInfo":{"status":"ok","timestamp":1733035302320,"user_tz":-330,"elapsed":381,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"f4da5ad4-9374-41b2-9302-18f57da2a1aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.         0.        ]\n"," [0.33333333 0.33333333]\n"," [0.66666667 0.66666667]\n"," [1.         1.        ]]\n"]}]},{"cell_type":"markdown","source":["fit_transform(X): First, it fits the scaler to the data (calculates the min and max values), and then it transforms the data based on those values, scaling it to the range [0, 1].\n"],"metadata":{"id":"VLuaP84b9Pcd"}},{"cell_type":"markdown","source":["2. Standardization (Z-score Normalization)\n","Standardization transforms the data to have a mean of 0 and a standard deviation of 1.\n"],"metadata":{"id":"DJ6C0_0hnAep"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Sample data\n","X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","X_standardized = scaler.fit_transform(X)\n","\n","print(X_standardized)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7HOVz5znKLJ","executionInfo":{"status":"ok","timestamp":1733298044659,"user_tz":-330,"elapsed":5490,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"1e1a22d2-ded9-467b-ce54-8b1a4dc64bcf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-1.34164079 -1.34164079]\n"," [-0.4472136  -0.4472136 ]\n"," [ 0.4472136   0.4472136 ]\n"," [ 1.34164079  1.34164079]]\n"]}]},{"cell_type":"markdown","source":["fit_transform(X): It computes the mean and standard deviation for each feature and then transforms the data to have a mean of 0 and a standard deviation of 1.\n"],"metadata":{"id":"zfCjE6dinXvQ"}},{"cell_type":"markdown","source":["3. Robust Scaling (Handling Outliers)\n","For data with outliers, RobustScaler can be used as it scales the features using the median and interquartile range (IQR), making it more robust to outliers.\n","\n"],"metadata":{"id":"X1ILIiu2ndF-"}},{"cell_type":"code","source":["from sklearn.preprocessing import RobustScaler\n","import numpy as np\n","\n","# Sample data with an outlier\n","X = np.array([[1, 2], [2, 3], [3, 4], [100, 500]])\n","\n","# Initialize the RobustScaler\n","scaler = RobustScaler()\n","\n","# Fit and transform the data\n","X_robust_scaled = scaler.fit_transform(X)\n","\n","print(X_robust_scaled)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJkxKwFMnNkp","executionInfo":{"status":"ok","timestamp":1733298098624,"user_tz":-330,"elapsed":357,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"958b2b3a-1979-4874-f529-b5d7a3330166"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.05882353 -0.01197605]\n"," [-0.01960784 -0.00399202]\n"," [ 0.01960784  0.00399202]\n"," [ 3.82352941  3.96407186]]\n"]}]},{"cell_type":"markdown","source":["4. Scaling a Single Feature\n","You can also scale individual features or columns of a dataset. For example, if you have a dataset with multiple features but only want to scale one feature:\n"],"metadata":{"id":"N1WMHgs0nkNT"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","\n","# Sample dataset\n","data = {'Feature1': [1, 2, 3, 4],\n","        'Feature2': [10, 20, 30, 40]}\n","\n","df = pd.DataFrame(data)\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Scale only 'Feature1'\n","df['Feature1_scaled'] = scaler.fit_transform(df[['Feature1']])\n","\n","print(df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2peDy0SKngnn","executionInfo":{"status":"ok","timestamp":1733298135665,"user_tz":-330,"elapsed":385,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"bd416b2d-fe0d-4217-86d1-c895f29b8519"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["   Feature1  Feature2  Feature1_scaled\n","0         1        10         0.000000\n","1         2        20         0.333333\n","2         3        30         0.666667\n","3         4        40         1.000000\n"]}]},{"cell_type":"markdown","source":["Q23. What is sklearn.preprocessing?"],"metadata":{"id":"jM8fEm9EnyPE"}},{"cell_type":"markdown","source":["The sklearn.preprocessing module in Scikit-learn provides tools to preprocess and transform raw data into a suitable format for machine learning models. It includes various functions and classes to handle tasks such as scaling, normalization, encoding categorical variables, generating polynomial features, and more.\n","\n","Preprocessing ensures that the data is clean, consistent, and standardized, which is crucial for the optimal performance of machine learning algorithms.\n","\n","Uses of sklearn.preprocessing\n","1. Improves Model Performance: Ensures data is on a consistent scale, which can significantly affect model performance and convergence.\n","2. Handles Missing and Categorical Data: Provides tools for imputing missing values and encoding categorical features.\n","3. Simplifies Feature Engineering: Automates common transformations and feature generation processes.\n","4. Prevents Bias: Ensures no single feature dominates due to its scale or representation.\n"],"metadata":{"id":"IuyY0dRZn7oy"}},{"cell_type":"markdown","source":["Q24. How do we split data for model fitting (training and testing) in Python?"],"metadata":{"id":"42G94s2jokV6"}},{"cell_type":"markdown","source":["In Python, train_test_split from the sklearn.model_selection module is commonly used to split a dataset into training and testing sets. This ensures that the model is trained on one portion of the data and evaluated on another, reducing the risk of overfitting.\n","\n","Steps to Split Data\n","\n","1. Import train_test_split: Import the function from sklearn.model_selection.\n","\n","2. Specify the Split Ratio: Decide what percentage of the data should be used for training (commonly 70-80%) and testing (commonly 20-30%).\n","\n","3. Shuffle the Data: By default, train_test_split shuffles the data before splitting to ensure randomness.\n","\n","4. Pass Your Features and Labels: Provide the features (X) and labels (y) for splitting.\n","\n"],"metadata":{"id":"5JRKAAHMotX7"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Example dataset\n","X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n","y = np.array([0, 1, 0, 1, 0])  # Labels\n","\n","# Split the data: 80% training, 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Print the results\n","print(\"Training Features:\\n\", X_train)\n","print(\"Testing Features:\\n\", X_test)\n","print(\"Training Labels:\\n\", y_train)\n","print(\"Testing Labels:\\n\", y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sV90N2i8np5p","executionInfo":{"status":"ok","timestamp":1733411724970,"user_tz":-330,"elapsed":8068,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"c2af8c39-eb88-4b3e-9dc6-b690663a800c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Features:\n"," [[ 9 10]\n"," [ 5  6]\n"," [ 1  2]\n"," [ 7  8]]\n","Testing Features:\n"," [[3 4]]\n","Training Labels:\n"," [0 0 0 1]\n","Testing Labels:\n"," [1]\n"]}]},{"cell_type":"markdown","source":["Q25. Explain data encoding?"],"metadata":{"id":"jwIe3UnxZtWi"}},{"cell_type":"markdown","source":["Data encoding is the process of converting categorical data into a numerical format that machine learning models can understand. Since most models work with numerical data, encoding ensures that categorical features can be used effectively in training.\n","\n","Types of Data Encoding:\n","1. Label Encoding\n","\n","Converts each unique category into a numerical label.\n","Suitable for ordinal data where the order of categories matters.\n"],"metadata":{"id":"j3CRVUuBZy7O"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","categories = ['Low', 'Medium', 'High', 'Low']\n","encoder = LabelEncoder()\n","encoded = encoder.fit_transform(categories)\n","print(encoded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKlpC0JWaAvT","executionInfo":{"status":"ok","timestamp":1733412052529,"user_tz":-330,"elapsed":353,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"d2a766e7-1218-4d85-a21c-c0543222b392"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 2 0 1]\n"]}]},{"cell_type":"markdown","source":["2. One-Hot Encoding\n","\n","Converts categories into binary vectors.\n","Suitable for nominal data where no order exists between categories."],"metadata":{"id":"MdlemxxKaTOf"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","\n","categories = np.array(['Low', 'Medium', 'High', 'Low']).reshape(-1, 1)\n","encoder = OneHotEncoder()\n","encoded = encoder.fit_transform(categories).toarray()\n","print(encoded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDZytnUyaHNN","executionInfo":{"status":"ok","timestamp":1733412108442,"user_tz":-330,"elapsed":375,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"03d65184-85f3-495c-9dfd-f02b98e9be2a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n"]}]},{"cell_type":"markdown","source":["3. Ordinal Encoding\n","\n","Assigns integers to categories based on a specified order.\n","Useful for ordinal data (e.g., \"Small\" < \"Medium\" < \"Large\")."],"metadata":{"id":"EuyICK5zai63"}},{"cell_type":"code","source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","categories = np.array(['Small', 'Medium', 'Large', 'Small']).reshape(-1, 1)\n","encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n","encoded = encoder.fit_transform(categories)\n","print(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkqQbW1NabSn","executionInfo":{"status":"ok","timestamp":1733412174212,"user_tz":-330,"elapsed":395,"user":{"displayName":"Vanshika Tomar","userId":"10467831022562400500"}},"outputId":"a8daa477-b253-4077-9f62-ecbb48806188"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.]\n"," [1.]\n"," [2.]\n"," [0.]]\n"]}]},{"cell_type":"markdown","source":["4. Binary Encoding\n","\n","Converts categories into binary format.\n","A mix of label encoding and one-hot encoding.\n","\n","Example:\n","\n","Category A → Label: 1 → Binary: 01.\n","Category B → Label: 2 → Binary: 10.\n"],"metadata":{"id":"tK-_CD9Bawpr"}},{"cell_type":"markdown","source":["5. Frequency Encoding\n","\n","Replaces categories with their frequency count in the dataset.\n","Useful when category occurrence is important.\n"],"metadata":{"id":"CjCphMf6a1qN"}}]}